{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1],[-2, -1],[1,1],[2,1]])\n",
    "Y = np.array([1,1,2,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "clf=SVC()\n",
    "clf.fit(X,Y)\n",
    "print(clf.fit(X,Y))\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   7.  15.  13.   1.   0.   0.]\n",
      " [  0.   8.  13.   6.  15.   4.   0.   0.]\n",
      " [  0.   2.   1.  13.  13.   0.   0.   0.]\n",
      " [  0.   0.   2.  15.  11.   1.   0.   0.]\n",
      " [  0.   0.   0.   1.  12.  12.   1.   0.]\n",
      " [  0.   0.   0.   0.   1.  10.   8.   0.]\n",
      " [  0.   0.   8.   4.   5.  14.   9.   0.]\n",
      " [  0.   0.   7.  13.  13.   9.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10985e080>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1091802e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ae3ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "\n",
    "digits = load_digits()\n",
    "# 数据纬度，1797幅图，8*8  \n",
    "# 显示一副图片  \n",
    "pl.gray()  \n",
    "print(digits.images[3])\n",
    "pl.matshow(digits.images[3])  \n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/envs/tensorflow3.5cpuconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-  \n",
    "import sys  \n",
    "from sklearn.datasets import load_digits  # 加载手写数字识别数据  \n",
    "import pylab as pl  \n",
    "from sklearn.cross_validation import train_test_split  # 训练测试数据分割  \n",
    "from sklearn.preprocessing import StandardScaler  # 标准化工具  \n",
    "from sklearn.svm import LinearSVC  \n",
    "from sklearn.metrics import classification_report  # 预测结果分析工具  \n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import shutil\n",
    "import xlrd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'ImageSynthesis/4lesions/syn/'\n",
    "data = xlrd.open_workbook('/Users/victor/Project4sxd/DRandDME/code/challenge/SubChallenge2-DRandDMEGrading/GROUNDTRUTH_LABELS/IDRiD_Training_Set.xlsx')\n",
    "table = data.sheets()[0]          #通过索引顺序获取\n",
    "nrows = table.nrows\n",
    "print(nrows)\n",
    "nameImage = table.col_values(0)\n",
    "\n",
    "grayfat = []\n",
    "DR_id = []\n",
    "DME_id = []\n",
    "for id in range(413):\n",
    "    File_Img_Name = 'IDRiD_%03d.png' % (id+1)\n",
    "    File_dir = data_dir + File_Img_Name\n",
    "    src = cv2.imread(File_dir)\n",
    "    src = cv2.resize(src, (536, 356), interpolation=cv2.INTER_NEAREST)\n",
    "    graysrc = rgb2gray(src)\n",
    "    print(id)\n",
    "\n",
    "    grayfatarr = graysrc.flatten().tolist()\n",
    "    grayfat.append(grayfatarr)\n",
    "    \n",
    "    DRGradingVale = table.cell(id+1,1).value\n",
    "    DR_id.append(DRGradingVale)\n",
    "    DMEGradingVale = table.cell(id+1,2).value\n",
    "    DME_id.append(DMEGradingVale)\n",
    "\n",
    "    \n",
    "if DR_id:\n",
    "    data1=np.array(grayfat)\n",
    "    drgradingvalue1 = np.array(DR_id, dtype=np.int64)\n",
    "    dmegradingvalue1 = np.array(DME_id, dtype=np.int64)\n",
    "    \n",
    "scipy.io.savemat('ImageSynthesis/4lesions/413Lesions1111.mat',\n",
    "                 {'data':data1,'drgrading':drgradingvalue1,'dmegrading':dmegradingvalue1})  # 写入mat文件 \n",
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'ImageSynthesis/4lesions/syn/'\n",
    "data = xlrd.open_workbook('/Users/victor/Project4sxd/DRandDME/code/challenge/SubChallenge2-DRandDMEGrading/GROUNDTRUTH_LABELS/IDRiD_Training_Set.xlsx')\n",
    "table = data.sheets()[0]          #通过索引顺序获取\n",
    "nrows = table.nrows\n",
    "print(nrows)\n",
    "nameImage = table.col_values(0)\n",
    "\n",
    "grayfat = []\n",
    "DR_id = []\n",
    "DME_id = []\n",
    "for id in range(20):\n",
    "    File_Img_Name = 'IDRiD_%03d.png' % (id+1)\n",
    "    File_dir = data_dir + File_Img_Name\n",
    "    src = cv2.imread(File_dir)\n",
    "    graysrc = rgb2gray(src)\n",
    "\n",
    "    grayfatarr = graysrc.flatten().tolist()\n",
    "    grayfat.append(grayfatarr)\n",
    "    \n",
    "    DRGradingVale = table.cell(id+1,1).value\n",
    "    DR_id.append(DRGradingVale)\n",
    "    DMEGradingVale = table.cell(id+1,2).value\n",
    "    DME_id.append(DMEGradingVale)\n",
    "\n",
    "    \n",
    "if DR_id:\n",
    "    data1=np.array(grayfat)\n",
    "    drgradingvalue1 = np.array(DR_id, dtype=np.int64)\n",
    "    dmegradingvalue1 = np.array(DME_id, dtype=np.int64)\n",
    "    \n",
    "scipy.io.savemat('ImageSynthesis/4lesions/413Lesions20.mat',\n",
    "                 {'data':data1,'drgrading':drgradingvalue1,'dmegrading':dmegradingvalue1})  # 写入mat文件 \n",
    "print(\"1\")\n",
    "\n",
    "grayfat = []\n",
    "DR_id = []\n",
    "DME_id = []\n",
    "\n",
    "for id in range(20,40):\n",
    "    File_Img_Name = 'IDRiD_%03d.png' % (id+1)\n",
    "    File_dir = data_dir + File_Img_Name\n",
    "    src = cv2.imread(File_dir)\n",
    "    graysrc = rgb2gray(src)\n",
    "    grayfatarr = graysrc.flatten().tolist()\n",
    "    grayfat.append(grayfatarr)\n",
    "    \n",
    "    DRGradingVale = table.cell(id+1,1).value\n",
    "    DR_id.append(DRGradingVale)\n",
    "    \n",
    "    DMEGradingVale = table.cell(id+1,2).value\n",
    "    DME_id.append(DMEGradingVale)\n",
    "    \n",
    "if DR_id:\n",
    "    data2=np.array(grayfat)\n",
    "    drgradingvalue2 = np.array(DR_id, dtype=np.int64)\n",
    "    dmegradingvalue2 = np.array(DME_id, dtype=np.int64)\n",
    "    \n",
    "scipy.io.savemat('ImageSynthesis/4lesions/413Lesions40.mat',\n",
    "                 {'data':data2,'drgrading':drgradingvalue2,'dmegrading':dmegradingvalue2})  # 写入mat文件 \n",
    "print(\"2\")\n",
    "\n",
    "grayfat = []\n",
    "DR_id = []\n",
    "DME_id = []\n",
    "\n",
    "for id in range(40,60):\n",
    "    File_Img_Name = 'IDRiD_%03d.png' % (id+1)\n",
    "    File_dir = data_dir + File_Img_Name\n",
    "    src = cv2.imread(File_dir)\n",
    "    graysrc = rgb2gray(src)\n",
    "    \n",
    "    grayfatarr = graysrc.flatten().tolist()\n",
    "    grayfat.append(grayfatarr)\n",
    "    \n",
    "    DRGradingVale = table.cell(id+1,1).value\n",
    "    DR_id.append(DRGradingVale)\n",
    "    \n",
    "    DMEGradingVale = table.cell(id+1,2).value\n",
    "    DME_id.append(DMEGradingVale)\n",
    "    \n",
    "if DR_id:\n",
    "    data3=np.array(grayfat)\n",
    "    drgradingvalue3 = np.array(DR_id, dtype=np.int64)\n",
    "    dmegradingvalue3 = np.array(DME_id, dtype=np.int64)\n",
    "    \n",
    "scipy.io.savemat('ImageSynthesis/4lesions/413Lesions60.mat',\n",
    "                 {'data':data3,'drgrading':drgradingvalue3,'dmegrading':dmegradingvalue3})  # 写入mat文件 \n",
    "print(\"3\")\n",
    "\n",
    "grayfat = []\n",
    "DR_id = []\n",
    "DME_id = []\n",
    "\n",
    "for id in range(60,80):\n",
    "    File_Img_Name = 'IDRiD_%03d.png' % (id+1)\n",
    "    File_dir = data_dir + File_Img_Name\n",
    "    src = cv2.imread(File_dir)\n",
    "    graysrc = rgb2gray(src)\n",
    "    \n",
    "    grayfatarr = graysrc.flatten().tolist()\n",
    "    grayfat.append(grayfatarr)\n",
    "    \n",
    "    DRGradingVale = table.cell(id+1,1).value\n",
    "    DR_id.append(DRGradingVale)\n",
    "    \n",
    "    DMEGradingVale = table.cell(id+1,2).value\n",
    "    DME_id.append(DMEGradingVale)\n",
    "    \n",
    "if DR_id:\n",
    "    data4=np.array(grayfat)\n",
    "    drgradingvalue4 = np.array(DR_id, dtype=np.int64)\n",
    "    dmegradingvalue4 = np.array(DME_id, dtype=np.int64)\n",
    "    \n",
    "scipy.io.savemat('ImageSynthesis/4lesions/413Lesions80.mat',\n",
    "                 {'data':data4,'drgrading':drgradingvalue4,'dmegrading':dmegradingvalue4})  # 写入mat文件 \n",
    "print(\"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'ImageSynthesis/4lesions/syn/'\n",
    "data = xlrd.open_workbook('/Users/victor/Project4sxd/DRandDME/code/challenge/SubChallenge2-DRandDMEGrading/GROUNDTRUTH_LABELS/IDRiD_Training_Set.xlsx')\n",
    "table = data.sheets()[0]          #通过索引顺序获取\n",
    "nrows = table.nrows\n",
    "print(nrows)\n",
    "nameImage = table.col_values(0)\n",
    "\n",
    "grayfat = []\n",
    "DR_id = []\n",
    "DME_id = []\n",
    "i = 21\n",
    "for id in range(1,414):\n",
    "    if id%20 == 0:\n",
    "        File_Img_Name = 'IDRiD_%03d.png' % (id)\n",
    "        File_dir = data_dir + File_Img_Name\n",
    "        src = cv2.imread(File_dir)\n",
    "        graysrc = rgb2gray(src)\n",
    "\n",
    "        grayfatarr = graysrc.flatten().tolist()\n",
    "        grayfat.append(grayfatarr)\n",
    "\n",
    "        DRGradingVale = table.cell(id,1).value\n",
    "        DR_id.append(DRGradingVale)\n",
    "        DMEGradingVale = table.cell(id,2).value\n",
    "        DME_id.append(DMEGradingVale)\n",
    "\n",
    "        data1=np.array(grayfat)\n",
    "        drgradingvalue1 = np.array(DR_id, dtype=np.int64)\n",
    "        dmegradingvalue1 = np.array(DME_id, dtype=np.int64)\n",
    "        \n",
    "        i = i+1\n",
    "        scipy.io.savemat('ImageSynthesis/4lesions/413Lesions_%02d.mat' % (i),\n",
    "                         {'data':data1,'drgrading':drgradingvalue1,'dmegrading':dmegradingvalue1})  # 写入mat文件 \n",
    "        print(i)\n",
    "        grayfat = []\n",
    "        DR_id = []\n",
    "        DME_id = []\n",
    "    elif id == 413:\n",
    "        File_Img_Name = 'IDRiD_%03d.png' % (id)\n",
    "        File_dir = data_dir + File_Img_Name\n",
    "        src = cv2.imread(File_dir)\n",
    "        graysrc = rgb2gray(src)\n",
    "\n",
    "        grayfatarr = graysrc.flatten().tolist()\n",
    "        grayfat.append(grayfatarr)\n",
    "\n",
    "        DRGradingVale = table.cell(id,1).value\n",
    "        DR_id.append(DRGradingVale)\n",
    "        DMEGradingVale = table.cell(id,2).value\n",
    "        DME_id.append(DMEGradingVale)\n",
    "\n",
    "        data1=np.array(grayfat)\n",
    "        drgradingvalue1 = np.array(DR_id, dtype=np.int64)\n",
    "        dmegradingvalue1 = np.array(DME_id, dtype=np.int64)\n",
    "        \n",
    "        i = i+1\n",
    "        scipy.io.savemat('ImageSynthesis/4lesions/413Lesions_%02d.mat' % (i),\n",
    "                         {'data':data1,'drgrading':drgradingvalue1,'dmegrading':dmegradingvalue1})  # 写入mat文件 \n",
    "        print(i)\n",
    "        grayfat = []\n",
    "        DR_id = []\n",
    "        DME_id = []\n",
    "    else:\n",
    "        File_Img_Name = 'IDRiD_%03d.png' % (id)\n",
    "        File_dir = data_dir + File_Img_Name\n",
    "        src = cv2.imread(File_dir)\n",
    "        graysrc = rgb2gray(src)\n",
    "\n",
    "        grayfatarr = graysrc.flatten().tolist()\n",
    "        grayfat.append(grayfatarr)\n",
    "\n",
    "        DRGradingVale = table.cell(id,1).value\n",
    "        DR_id.append(DRGradingVale)\n",
    "        DMEGradingVale = table.cell(id,2).value\n",
    "        DME_id.append(DMEGradingVale)\n",
    "\n",
    "        data1=np.array(grayfat)\n",
    "        drgradingvalue1 = np.array(DR_id, dtype=np.int64)\n",
    "        dmegradingvalue1 = np.array(DME_id, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert element 0 ([ 0.  0.  0.  0.  0.]) to hsize_t",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mh5py/utils.pyx\u001b[0m in \u001b[0;36mh5py.utils.convert_tuple\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d1844d1ae7fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ImageSynthesis/4lesions/413Lesions.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# 写入\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drgrading'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrgradingvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dmegrading'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdmegradingvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow3.5cpuconda/lib/python3.5/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow3.5cpuconda/lib/python3.5/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0msid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNULL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0msid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5s.pyx\u001b[0m in \u001b[0;36mh5py.h5s.create_simple\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/utils.pyx\u001b[0m in \u001b[0;36mh5py.utils.convert_tuple\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert element 0 ([ 0.  0.  0.  0.  0.]) to hsize_t"
     ]
    }
   ],
   "source": [
    "if DR_id:\n",
    "    #data1 = np.array(grayfat)\n",
    "    data = np.stack(grayfat, axis=1)\n",
    "    drgradingvalue = np.array(DR_id, dtype=np.int64)\n",
    "    dmegradingvalue = np.array(DME_id, dtype=np.int64)\n",
    "    \n",
    "#data=np.array(grayfat)\n",
    "file = h5py.File('ImageSynthesis/4lesions/413Lesions.h5','w')  \n",
    "# 写入  \n",
    "file.create_dataset('data',data)  \n",
    "file.create_dataset('drgrading',drgradingvalue)  \n",
    "file.create_dataset('dmegrading',dmegradingvalue)  \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 12212224)\n",
      "[[2 1 1 2 2 0 2 1 2 2 2 2 2]]\n",
      "int64\n",
      "[[1 0 0 0 1 0 1 0 1 0 0 0 0]]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "mat1data = scipy.io.loadmat('ImageSynthesis/4lesions/413Lesions_21.mat')\n",
    "grayfat1 = mat1data['data']\n",
    "drgrading1 = mat1data['drgrading']\n",
    "dmegrading1 = mat1data['dmegrading']\n",
    "\n",
    "print(grayfat1.shape)\n",
    "print(drgrading1)\n",
    "print(drgrading1.dtype)\n",
    "print(dmegrading1)\n",
    "print(dmegrading1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "(413, 763264)\n",
      "[3 3 2 3 4 4 4 4 3 4 3 3 3 4 4 2 4 2 2 2 1 4 3 4 3 3 4 4 2 4 4 4 4 3 3 3 2\n",
      " 2 3 3 2 2 2 2 2 4 2 2 3 2 2 2 4 4 3 2 3 2 2 2 2 4 2 2 3 3 4 2 2 4 2 2 2 2\n",
      " 3 2 3 2 1 2 4 2 2 2 2 4 2 3 4 4 2 2 2 2 3 4 2 3 3 4 3 2 2 4 1 2 2 3 2 3 4\n",
      " 3 2 3 3 2 2 0 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 3 0 0 2 0 2\n",
      " 2 2 2 4 0 2 3 0 1 0 2 0 1 0 0 2 0 1 0 0 0 3 2 0 0 0 0 0 0 2 2 0 0 0 0 0 0\n",
      " 0 4 0 0 0 0 0 0 2 2 0 0 0 2 2 0 4 4 2 2 0 0 2 4 2 2 0 0 0 2 3 4 0 1 0 0 0\n",
      " 2 2 2 2 0 1 1 0 0 3 2 3 4 0 0 0 0 3 3 1 2 2 2 2 2 4 4 0 0 4 1 1 0 2 2 0 3\n",
      " 0 0 3 2 1 1 2 1 0 3 0 3 3 2 2 0 4 3 0 0 0 0 2 3 3 2 3 3 2 2 3 2 3 3 0 2 2\n",
      " 3 0 0 2 2 3 2 3 0 2 3 0 0 3 2 3 2 0 0 0 3 0 3 3 0 0 2 4 4 2 3 0 3 0 3 3 0\n",
      " 3 3 0 2 2 3 3 0 1 3 0 0 3 3 3 0 0 1 4 4 2 4 0 4 0 0 2 2 0 0 2 1 1 2 2 0 2\n",
      " 1 2 2 2 2 2]\n",
      "(413,)\n",
      "[2 2 2 2 0 1 0 2 2 1 1 2 0 2 2 2 2 2 2 2 0 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 1\n",
      " 0 2 2 2 2 0 2 0 0 2 2 2 2 2 1 2 2 2 2 2 2 2 1 0 2 2 2 0 2 1 1 2 0 2 2 1 2\n",
      " 2 2 2 2 0 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 1 2 2 2 2 2 2 2 0 2 2 1 2 2 2\n",
      " 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 1 2 0 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 2 0 2\n",
      " 2 1 1 2 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 2 0 0 0 0 0 0 2 2 0 0 0 2 2 0 2 2 2 2 0 0 2 2 2 2 0 0 0 1 1 2 0 0 0 0 0\n",
      " 2 2 1 1 0 0 0 0 0 0 1 2 2 0 0 0 0 2 2 0 2 2 1 2 2 2 2 0 0 2 0 0 0 2 2 0 2\n",
      " 0 0 2 2 0 0 1 0 0 2 0 2 2 2 0 0 2 2 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 1\n",
      " 2 0 0 2 2 2 1 2 0 2 2 0 0 2 2 2 2 0 0 0 2 0 2 2 0 0 2 2 2 1 2 0 2 0 2 2 0\n",
      " 2 2 0 1 1 2 2 0 0 2 0 0 2 2 2 0 0 0 2 2 0 2 0 2 0 0 0 1 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 0]\n",
      "(413,)\n"
     ]
    }
   ],
   "source": [
    "data1 = scipy.io.loadmat('ImageSynthesis/4lesions/413Lesions2222.mat')\n",
    "data2 = scipy.io.loadmat('ImageSynthesis/4lesions/413Lesions2223.mat')\n",
    "grayfat1 = data1['data']\n",
    "drgrading1 = data1['drgrading']\n",
    "dmegrading1 = data1['dmegrading']\n",
    "\n",
    "grayfat2 = data2['data']\n",
    "drgrading2 = data2['drgrading']\n",
    "dmegrading2 = data2['dmegrading']\n",
    "\n",
    "#newgrayfat = np.stack(grayfat1,grayfat2)\n",
    "newgrayfat = np.concatenate((grayfat1,grayfat2),axis=0)\n",
    "print(newgrayfat)\n",
    "print(newgrayfat.shape)\n",
    "\n",
    "newdrgrading = np.append(drgrading1,drgrading2)\n",
    "print(newdrgrading)\n",
    "print(newdrgrading.shape)\n",
    "\n",
    "newdmegrading = np.append(dmegrading1,dmegrading2)\n",
    "print(newdmegrading)\n",
    "print(newdmegrading.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "(413, 12212224)\n",
      "[3 3 2 3 4 4 4 4 3 4 3 3 3 4 4 2 4 2 2 2 1 4 3 4 3 3 4 4 2 4 4 4 4 3 3 3 2\n",
      " 2 3 3 2 2 2 2 2 4 2 2 3 2 2 2 4 4 3 2 3 2 2 2 2 4 2 2 3 3 4 2 2 4 2 2 2 2\n",
      " 3 2 3 2 1 2 4 2 2 2 2 4 2 3 4 4 2 2 2 2 3 4 2 3 3 4 3 2 2 4 1 2 2 3 2 3 4\n",
      " 3 2 3 3 2 2 0 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 3 0 0 2 0 2\n",
      " 2 2 2 4 0 2 3 0 1 0 2 0 1 0 0 2 0 1 0 0 0 3 2 0 0 0 0 0 0 2 2 0 0 0 0 0 0\n",
      " 0 4 0 0 0 0 0 0 2 2 0 0 0 2 2 0 4 4 2 2 0 0 2 4 2 2 0 0 0 2 3 4 0 1 0 0 0\n",
      " 2 2 2 2 0 1 1 0 0 3 2 3 4 0 0 0 0 3 3 1 2 2 2 2 2 4 4 0 0 4 1 1 0 2 2 0 3\n",
      " 0 0 3 2 1 1 2 1 0 3 0 3 3 2 2 0 4 3 0 0 0 0 2 3 3 2 3 3 2 2 3 2 3 3 0 2 2\n",
      " 3 0 0 2 2 3 2 3 0 2 3 0 0 3 2 3 2 0 0 0 3 0 3 3 0 0 2 4 4 2 3 0 3 0 3 3 0\n",
      " 3 3 0 2 2 3 3 0 1 3 0 0 3 3 3 0 0 1 4 4 2 4 0 4 0 0 2 2 0 0 2 1 1 2 2 0 2\n",
      " 1 2 2 2 2 2]\n",
      "(413,)\n",
      "[2 2 2 2 0 1 0 2 2 1 1 2 0 2 2 2 2 2 2 2 0 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 1\n",
      " 0 2 2 2 2 0 2 0 0 2 2 2 2 2 1 2 2 2 2 2 2 2 1 0 2 2 2 0 2 1 1 2 0 2 2 1 2\n",
      " 2 2 2 2 0 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 1 2 2 2 2 2 2 2 0 2 2 1 2 2 2\n",
      " 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 1 2 0 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 2 0 2\n",
      " 2 1 1 2 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 2 0 0 0 0 0 0 2 2 0 0 0 2 2 0 2 2 2 2 0 0 2 2 2 2 0 0 0 1 1 2 0 0 0 0 0\n",
      " 2 2 1 1 0 0 0 0 0 0 1 2 2 0 0 0 0 2 2 0 2 2 1 2 2 2 2 0 0 2 0 0 0 2 2 0 2\n",
      " 0 0 2 2 0 0 1 0 0 2 0 2 2 2 0 0 2 2 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 1\n",
      " 2 0 0 2 2 2 1 2 0 2 2 0 0 2 2 2 2 0 0 0 2 0 2 2 0 0 2 2 2 1 2 0 2 0 2 2 0\n",
      " 2 2 0 1 1 2 2 0 0 2 0 0 2 2 2 0 0 0 2 2 0 2 0 2 0 0 0 1 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 0]\n",
      "(413,)\n"
     ]
    }
   ],
   "source": [
    "data1 = scipy.io.loadmat('ImageSynthesis/4lesions/413Lesions_01.mat')\n",
    "grayfat1 = data1['data']\n",
    "drgrading1 = data1['drgrading']\n",
    "dmegrading1 = data1['dmegrading']\n",
    "for i in range(2,22):\n",
    "    data2 = scipy.io.loadmat('ImageSynthesis/4lesions/413Lesions_%02d.mat' % i )\n",
    "    grayfat2 = data2['data']\n",
    "    drgrading2 = data2['drgrading']\n",
    "    dmegrading2 = data2['dmegrading']\n",
    "    grayfat1 = np.concatenate((grayfat1,grayfat2),axis=0)\n",
    "    drgrading1 = np.append(drgrading1,drgrading2)\n",
    "    dmegrading1 = np.append(dmegrading1,dmegrading2)\n",
    "    print(i)\n",
    "    \n",
    "print(grayfat1.shape)\n",
    "print(drgrading1)\n",
    "print(drgrading1.shape)\n",
    "print(dmegrading1)\n",
    "print(dmegrading1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = grayfat1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa = grayfat1.tolist()\n",
    "file = h5py.File('ImageSynthesis/4lesions/413Lesions.h5','w')  \n",
    "# 写入  \n",
    "file.create_dataset('data',aa)  \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = newgrayfat\n",
    "target = newdrgrading\n",
    "target_names=np.array([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_IDRiD():\n",
    "    grayfat = []\n",
    "    data_dir = 'ImageSynthesis/4lesions/syn/'\n",
    "#     matdata = scipy.io.loadmat('ImageSynthesis/4lesions/413Lesions20.mat')\n",
    "#     grayfat1 = matdata['data']\n",
    "#     print(grayfat1.shape)\n",
    "#     target = matdata['drgrading']\n",
    "#     print(target)\n",
    "#     dmegrading1 = matdata['dmegrading']\n",
    "#     for id in range(30):\n",
    "#         File_Img_Name = 'IDRiD_%03d.png' % (id+1)\n",
    "#         File_dir = data_dir + File_Img_Name\n",
    "#         #print(File_dir)\n",
    "#         src = cv2.imread(File_dir)\n",
    "#         graysrc = rgb2gray(src)\n",
    "# #         print(type(graysrc))\n",
    "# #         print(graysrc.shape)\n",
    "# #         print(graysrc.dtype)\n",
    "#         #print(src1[id])\n",
    "#         grayfatarr = graysrc.flatten().tolist()\n",
    "#         grayfat.append(grayfatarr)\n",
    "#         #src1[id] = src2\n",
    "#         #print(src1.shape)\n",
    "#         #print(grayfat[id])\n",
    "#         print(id)\n",
    "        \n",
    "    grayfat1=np.array(grayfat)\n",
    "    target_names=np.array([0,1,2,3,4])\n",
    "    target=np.array([1,2,3,4,0,1,2,3,4,0,1,2,3,4,0,1,2,3,4,0,1,2,3,4,0,1,2,3,4,0])\n",
    "    return grayfat1,target,target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "a=np.array(grayfat)\n",
    "print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "data,target,target_names = load_IDRiD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size=0.33, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "(2848, 4288)\n",
      "float64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Bunch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2fa6ba211d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mIDRiD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_IDRiD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# 分割数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIDRiD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDRiD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-26d2d708398b>\u001b[0m in \u001b[0;36mload_IDRiD\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrayfat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mheader_physiological\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     return Bunch(data=a, \n\u001b[0m\u001b[1;32m     35\u001b[0m                  \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                  target_names=header_physiological)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'Bunch' is not defined"
     ]
    }
   ],
   "source": [
    "IDRiD = load_IDRiD()\n",
    "# 分割数据\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(IDRiD.data, IDRiD.target, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1797, 64)\n",
      "[  0.   0.   7.  15.  13.   1.   0.   0.   0.   8.  13.   6.  15.   4.   0.\n",
      "   0.   0.   2.   1.  13.  13.   0.   0.   0.   0.   0.   2.  15.  11.   1.\n",
      "   0.   0.   0.   0.   0.   1.  12.  12.   1.   0.   0.   0.   0.   0.   1.\n",
      "  10.   8.   0.   0.   0.   8.   4.   5.  14.   9.   0.   0.   0.   7.  13.\n",
      "  13.   9.   0.   0.]\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "# 数据纬度，1797幅图，8*8\n",
    "#print(digits)\n",
    "print(type(digits.data))\n",
    "print(digits.data.shape)\n",
    "\n",
    "print(digits.data[3])\n",
    "print(type(digits.data[3]))\n",
    "print(digits.data[3].dtype)\n",
    "\n",
    "print(digits.target[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      1.00      0.61        44\n",
      "          1       0.17      0.20      0.18         5\n",
      "          2       0.55      0.13      0.21        46\n",
      "          3       0.30      0.10      0.15        30\n",
      "          4       0.11      0.08      0.10        12\n",
      "\n",
      "avg / total       0.40      0.40      0.31       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "# fit是实例方法，必须由实例调用\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(X_train, Y_train)\n",
    "\n",
    "Y_predict = lsvc.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, Y_predict, target_names=target_names.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_cross_validation(train_x, train_y):  \n",
    "    from sklearn.grid_search import GridSearchCV  \n",
    "    from sklearn.svm import SVC  \n",
    "    model = SVC(kernel='rbf', probability=True)  \n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}  \n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)  \n",
    "    grid_search.fit(train_x, train_y)  \n",
    "    best_parameters = grid_search.best_estimator_.get_params()  \n",
    "    for para, val in best_parameters.items():  \n",
    "        print(para, val)  \n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train, num_feat = X_train.shape  \n",
    "num_test, num_feat = X_test.shape  \n",
    "is_binary_class = (len(np.unique(Y_train)) == 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifiers = ['NB', 'KNN', 'LR', 'RF', 'DT', 'SVM','SVMCV', 'GBDT']  \n",
    "classifiers = {#'NB':naive_bayes_classifier,  \n",
    "              #'KNN':knn_classifier,  \n",
    "              # 'LR':logistic_regression_classifier,  \n",
    "              # 'RF':random_forest_classifier,  \n",
    "              # 'DT':decision_tree_classifier,  \n",
    "             # 'SVM':svm_classifier,  \n",
    "            'SVMCV':svm_cross_validation,  \n",
    "             #'GBDT':gradient_boosting_classifier  \n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/envs/tensorflow3.5cpuconda/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import pickle as pickle  \n",
    "model_save_file = None \n",
    "start_time = time.time()  \n",
    "model = classifiers['SVMCV'](X_train, Y_train)  \n",
    "print('training took %fs!' % (time.time() - start_time))\n",
    "predict = model.predict(test_x)  \n",
    "if model_save_file != None:  \n",
    "    model_save['SVMCV'] = model  \n",
    "if is_binary_class:  \n",
    "    precision = metrics.precision_score(Y_test, predict)  \n",
    "    recall = metrics.recall_score(Y_test, predict)  \n",
    "    print('precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))\n",
    "accuracy = metrics.accuracy_score(Y_test, predict)  \n",
    "print('accuracy: %.2f%%' % (100 * accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
